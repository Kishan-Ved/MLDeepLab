{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1VYVXlfzx7rI4hLcz5S2a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kishan-Ved/MLDeepLab/blob/main/Vanilla_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla Autoencoder from Scratch\n"
      ],
      "metadata": {
        "id": "14Oryb6jDzQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Necessary Imports"
      ],
      "metadata": {
        "id": "5vYnzXofD5-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "wq9bTiLTD9wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Functions"
      ],
      "metadata": {
        "id": "ae6dQQG_D-aj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanilla function to compute linear and ReLU\n",
        "\n",
        "Returns encoded and decoded data"
      ],
      "metadata": {
        "id": "tlSwiWBNEFz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vanilla(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Vanilla, self).__init__()\n",
        "\n",
        "        self.layer1 = Linear(input_size, hidden_size)\n",
        "        self.relu = ReLU()\n",
        "        self.layer2 = Linear(hidden_size, output_size)\n",
        "        self.layer3 = Linear(output_size, hidden_size)\n",
        "        self.layer4 = Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        encoded = self.layer2(x)\n",
        "        x = self.layer3(encoded)\n",
        "        decoded = self.layer4(x)\n",
        "        return encoded, decoded"
      ],
      "metadata": {
        "id": "MpKeykicEL4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear function to perform calculations and return the matrix with bias"
      ],
      "metadata": {
        "id": "5Xi1qMouEOQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.Tensor(input_size, output_size))\n",
        "        self.bias = nn.Parameter(torch.Tensor(output_size))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.matmul(x, self.weight) + self.bias"
      ],
      "metadata": {
        "id": "RdEtwGF3Er5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU function"
      ],
      "metadata": {
        "id": "aqFdgvYkEvMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.max(torch.zeros_like(x), x)"
      ],
      "metadata": {
        "id": "zEETw1TsEzZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "QQENeI1oE5Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can perform both encoding and decoding.\n",
        "\n",
        "To test the model, input data is a random array, which is passed to the model.\n",
        "\n",
        "First, the model enocdes the array.\n",
        "\n",
        "Next, we use the model to decode the encoded array.\n",
        "\n",
        "The loss between the actual data and the decoded data is measured.\n",
        "\n",
        "Outputs:\n",
        "\n",
        "---\n",
        "Actual array\n",
        "\n",
        "Decoded array\n",
        "\n",
        "Loss"
      ],
      "metadata": {
        "id": "-L4zAw7fFUOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784  # Input size\n",
        "hidden_size = 256  # Number of neurons in the hidden layer\n",
        "output_size = 64  # Size of the encoded representation\n",
        "\n",
        "encoder = Vanilla(input_size, hidden_size, output_size)\n",
        "\n",
        "batch_size = 32\n",
        "input_data = torch.randn(batch_size, input_size)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate) # Create an optimizer\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs): # Training loop\n",
        "    encoded_data, decoded_data = encoder(input_data) # Forward pass\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "    reconstruction_loss = loss_fn(decoded_data, input_data)\n",
        "\n",
        "    optimizer.zero_grad() # Backward pass and optimization\n",
        "    reconstruction_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Reconstruction Loss: {reconstruction_loss.item()}\")\n",
        "\n",
        "encoded_data, decoded_data = encoder(input_data)\n",
        "\n",
        "input_data_np = input_data.numpy()\n",
        "decoded_data_np = decoded_data.detach().numpy()\n",
        "print()\n",
        "print(\"Input data:\")\n",
        "print(input_data_np)\n",
        "print()\n",
        "print(\"Decoded data:\")\n",
        "print(decoded_data_np)\n",
        "reconstruction_loss = loss_fn(decoded_data, input_data)\n",
        "print()\n",
        "print(\"Final Reconstruction Loss:\", reconstruction_loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-8t6eroE7In",
        "outputId": "41d08388-5350-482f-f5b3-46d32caf783c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Reconstruction Loss: 1.0066027641296387\n",
            "Epoch [2/100], Reconstruction Loss: 0.9777428507804871\n",
            "Epoch [3/100], Reconstruction Loss: 0.9522109627723694\n",
            "Epoch [4/100], Reconstruction Loss: 0.922700822353363\n",
            "Epoch [5/100], Reconstruction Loss: 0.8868919014930725\n",
            "Epoch [6/100], Reconstruction Loss: 0.845042884349823\n",
            "Epoch [7/100], Reconstruction Loss: 0.7988901138305664\n",
            "Epoch [8/100], Reconstruction Loss: 0.7497849464416504\n",
            "Epoch [9/100], Reconstruction Loss: 0.6977936625480652\n",
            "Epoch [10/100], Reconstruction Loss: 0.6434291005134583\n",
            "Epoch [11/100], Reconstruction Loss: 0.5888732671737671\n",
            "Epoch [12/100], Reconstruction Loss: 0.5367803573608398\n",
            "Epoch [13/100], Reconstruction Loss: 0.48876625299453735\n",
            "Epoch [14/100], Reconstruction Loss: 0.4447524845600128\n",
            "Epoch [15/100], Reconstruction Loss: 0.403629332780838\n",
            "Epoch [16/100], Reconstruction Loss: 0.36431753635406494\n",
            "Epoch [17/100], Reconstruction Loss: 0.32629090547561646\n",
            "Epoch [18/100], Reconstruction Loss: 0.28986361622810364\n",
            "Epoch [19/100], Reconstruction Loss: 0.25572624802589417\n",
            "Epoch [20/100], Reconstruction Loss: 0.22443681955337524\n",
            "Epoch [21/100], Reconstruction Loss: 0.19612860679626465\n",
            "Epoch [22/100], Reconstruction Loss: 0.17068246006965637\n",
            "Epoch [23/100], Reconstruction Loss: 0.1478099673986435\n",
            "Epoch [24/100], Reconstruction Loss: 0.1273690015077591\n",
            "Epoch [25/100], Reconstruction Loss: 0.10934822261333466\n",
            "Epoch [26/100], Reconstruction Loss: 0.09374498575925827\n",
            "Epoch [27/100], Reconstruction Loss: 0.08049853891134262\n",
            "Epoch [28/100], Reconstruction Loss: 0.06938526779413223\n",
            "Epoch [29/100], Reconstruction Loss: 0.060058191418647766\n",
            "Epoch [30/100], Reconstruction Loss: 0.052174508571624756\n",
            "Epoch [31/100], Reconstruction Loss: 0.04546499624848366\n",
            "Epoch [32/100], Reconstruction Loss: 0.03972896933555603\n",
            "Epoch [33/100], Reconstruction Loss: 0.03485789895057678\n",
            "Epoch [34/100], Reconstruction Loss: 0.030788572505116463\n",
            "Epoch [35/100], Reconstruction Loss: 0.02744939923286438\n",
            "Epoch [36/100], Reconstruction Loss: 0.024760935455560684\n",
            "Epoch [37/100], Reconstruction Loss: 0.022602837532758713\n",
            "Epoch [38/100], Reconstruction Loss: 0.02079853229224682\n",
            "Epoch [39/100], Reconstruction Loss: 0.019153445959091187\n",
            "Epoch [40/100], Reconstruction Loss: 0.017537174746394157\n",
            "Epoch [41/100], Reconstruction Loss: 0.015925439074635506\n",
            "Epoch [42/100], Reconstruction Loss: 0.014380265027284622\n",
            "Epoch [43/100], Reconstruction Loss: 0.012975490652024746\n",
            "Epoch [44/100], Reconstruction Loss: 0.011756796389818192\n",
            "Epoch [45/100], Reconstruction Loss: 0.01072249747812748\n",
            "Epoch [46/100], Reconstruction Loss: 0.009842053055763245\n",
            "Epoch [47/100], Reconstruction Loss: 0.009072099812328815\n",
            "Epoch [48/100], Reconstruction Loss: 0.0083706583827734\n",
            "Epoch [49/100], Reconstruction Loss: 0.0077037508599460125\n",
            "Epoch [50/100], Reconstruction Loss: 0.007047886960208416\n",
            "Epoch [51/100], Reconstruction Loss: 0.0063914284110069275\n",
            "Epoch [52/100], Reconstruction Loss: 0.005731439217925072\n",
            "Epoch [53/100], Reconstruction Loss: 0.005076543428003788\n",
            "Epoch [54/100], Reconstruction Loss: 0.004446970298886299\n",
            "Epoch [55/100], Reconstruction Loss: 0.00387004716321826\n",
            "Epoch [56/100], Reconstruction Loss: 0.00336994300596416\n",
            "Epoch [57/100], Reconstruction Loss: 0.0029590337071567774\n",
            "Epoch [58/100], Reconstruction Loss: 0.002635151380673051\n",
            "Epoch [59/100], Reconstruction Loss: 0.002382677746936679\n",
            "Epoch [60/100], Reconstruction Loss: 0.0021774310152977705\n",
            "Epoch [61/100], Reconstruction Loss: 0.0019951388239860535\n",
            "Epoch [62/100], Reconstruction Loss: 0.0018195391166955233\n",
            "Epoch [63/100], Reconstruction Loss: 0.0016452650306746364\n",
            "Epoch [64/100], Reconstruction Loss: 0.0014746621018275619\n",
            "Epoch [65/100], Reconstruction Loss: 0.0013134002219885588\n",
            "Epoch [66/100], Reconstruction Loss: 0.001167179667390883\n",
            "Epoch [67/100], Reconstruction Loss: 0.0010398351587355137\n",
            "Epoch [68/100], Reconstruction Loss: 0.0009326907456852496\n",
            "Epoch [69/100], Reconstruction Loss: 0.0008451375178992748\n",
            "Epoch [70/100], Reconstruction Loss: 0.0007749915821477771\n",
            "Epoch [71/100], Reconstruction Loss: 0.0007183087873272598\n",
            "Epoch [72/100], Reconstruction Loss: 0.000669750792440027\n",
            "Epoch [73/100], Reconstruction Loss: 0.0006238988717086613\n",
            "Epoch [74/100], Reconstruction Loss: 0.0005766325630247593\n",
            "Epoch [75/100], Reconstruction Loss: 0.0005260186153464019\n",
            "Epoch [76/100], Reconstruction Loss: 0.00047255700337700546\n",
            "Epoch [77/100], Reconstruction Loss: 0.0004187803715467453\n",
            "Epoch [78/100], Reconstruction Loss: 0.0003681061207316816\n",
            "Epoch [79/100], Reconstruction Loss: 0.00032353229471482337\n",
            "Epoch [80/100], Reconstruction Loss: 0.0002867362345568836\n",
            "Epoch [81/100], Reconstruction Loss: 0.0002576774568296969\n",
            "Epoch [82/100], Reconstruction Loss: 0.00023478796356357634\n",
            "Epoch [83/100], Reconstruction Loss: 0.00021575993741862476\n",
            "Epoch [84/100], Reconstruction Loss: 0.00019843173504341394\n",
            "Epoch [85/100], Reconstruction Loss: 0.0001812941482057795\n",
            "Epoch [86/100], Reconstruction Loss: 0.0001636812958167866\n",
            "Epoch [87/100], Reconstruction Loss: 0.00014574985834769905\n",
            "Epoch [88/100], Reconstruction Loss: 0.00012819335097447038\n",
            "Epoch [89/100], Reconstruction Loss: 0.00011190365330548957\n",
            "Epoch [90/100], Reconstruction Loss: 9.777885134099051e-05\n",
            "Epoch [91/100], Reconstruction Loss: 8.647112554172054e-05\n",
            "Epoch [92/100], Reconstruction Loss: 7.804697088431567e-05\n",
            "Epoch [93/100], Reconstruction Loss: 7.19432791811414e-05\n",
            "Epoch [94/100], Reconstruction Loss: 6.725786806782708e-05\n",
            "Epoch [95/100], Reconstruction Loss: 6.30258145974949e-05\n",
            "Epoch [96/100], Reconstruction Loss: 5.844685438205488e-05\n",
            "Epoch [97/100], Reconstruction Loss: 5.3173662308836356e-05\n",
            "Epoch [98/100], Reconstruction Loss: 4.743056706502102e-05\n",
            "Epoch [99/100], Reconstruction Loss: 4.178254312137142e-05\n",
            "Epoch [100/100], Reconstruction Loss: 3.6783836549147964e-05\n",
            "\n",
            "Input data:\n",
            "[[ 0.25693637  1.3352896  -0.9880017  ...  0.81738704 -0.1472403\n",
            "   0.13577774]\n",
            " [ 1.4433427   1.2979369  -0.05215636 ... -0.01188001  0.74774516\n",
            "  -3.044344  ]\n",
            " [-0.97059196 -0.6659614   0.8002503  ... -0.42442587 -0.42620167\n",
            "  -0.7775219 ]\n",
            " ...\n",
            " [ 0.7968462  -0.22144821  0.96952915 ... -0.17210533  0.82137865\n",
            "   1.0965644 ]\n",
            " [-0.2101599   0.83155173 -0.42877752 ...  0.9389729   0.3562018\n",
            "   0.24402893]\n",
            " [ 0.6731628  -1.1825451   0.23824169 ... -0.01074806  0.44254777\n",
            "  -0.8378325 ]]\n",
            "\n",
            "Decoded data:\n",
            "[[ 0.26629278  1.3397387  -0.98769885 ...  0.8228277  -0.15213063\n",
            "   0.13525863]\n",
            " [ 1.4460374   1.3021641  -0.05355614 ... -0.01636676  0.7471383\n",
            "  -3.0440602 ]\n",
            " [-0.9638929  -0.67142373  0.7925779  ... -0.42108214 -0.43235636\n",
            "  -0.77075315]\n",
            " ...\n",
            " [ 0.7966319  -0.23467281  0.9658495  ... -0.16718727  0.82325214\n",
            "   1.109568  ]\n",
            " [-0.19921468  0.81872153 -0.42422608 ...  0.93721396  0.35669822\n",
            "   0.23559795]\n",
            " [ 0.6652166  -1.1769542   0.22938825 ... -0.01016041  0.44561836\n",
            "  -0.8412716 ]]\n",
            "\n",
            "Final Reconstruction Loss: 3.274644768680446e-05\n"
          ]
        }
      ]
    }
  ]
}